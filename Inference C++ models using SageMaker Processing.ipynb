{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning has been existing for decades. Before the prevalence of doing machine learning with Python, many other languages such as Java, C++ were used to build models. Refactoring legacy models in C++ or Java could be forbiddingly expensive and time consuming. In this blog, we are going demonstrate inferencing C++ models by building a customer container first and then run a ScriptProcessor jobs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C++ model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a simple C++ test file for demonstration purpose. This C++ program accepts input data as a series of strings separated by comma. For example, “2,3“ represent a row of input data 2 and 3 in two separate columns. Each row has two column. The C++ code will parse out numeric values and run inference. We use a simple linear regression model y=x1 + x2 in this example. Customer can modify the C++ inference code to inference more realistic and complicated models. The C++ program will print out result (i.e., y in this case) to standard output stream. The complete Python script is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize test.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `g++` to compile the `test.cpp` file into an executable file `a.out`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ -std=c++11 test.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run a quick test on `a.out` to make sure it works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "./a.out '9,8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker Processing is a new capability of Amazon SageMaker (https://aws.amazon.com/sagemaker/) for running processing and model evaluation workloads with a fully managed experience. Amazon SageMaker Processing lets customers run analytics jobs for data engineering and model evaluation on Amazon SageMaker easily and at scale. SageMaker Processing allows customers to enjoy the benefits of a fully managed environment with all the security and compliance guarantees built into Amazon SageMaker. With Amazon SageMaker Processing, customers have the flexibility of using the built-in data processing containers or bringing their own containers and submitting custom jobs to run on managed infrastructure. Once submitted, Amazon SageMaker launches the compute instances, processes and analyzes the input data and releases the resources upon completion. \n",
    "\n",
    "The processing container is defined as shown below. We have Anaconda and Pandas installed into the container. `a.out` is the C++ executable that contains the model inference logic. `processing.py` is the Python script we use to call C++ executable and save results. We build the Docker container and push it to Amazon Elastic Container Registry. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=cpp_processing\n",
    "\n",
    "#cd container\n",
    "\n",
    "chmod +x process_script.py\n",
    "chmod +x a.out\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build  -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Processing script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use the Amazon SageMaker Python SDK to submit a processing job. Use the container that was just built, and `process_script.py` script for calling the C++ model.\n",
    "\n",
    "The `process_script.py` first finds all data files under `/opt/ml/processing/input/`. These data files are downloaded by SageMaker from S3 to designated local directory in the container. By default, when you use multiple instances, data from S3 are duplicated to each container instance. That means every instance get the full dataset. By setting `s3_data_distribution_type='ShardedByS3Key'`, each instance gets approximately 1/n of the number of total input date files. \n",
    "\n",
    "We read each data file into memory and convert it into a long string ready for C++ executable to consume. `subprocess` module from Python allows us to run the C++ executable and connect to output and error pipes. Output is saved as csv file to `/opt/ml/processing/output`. Upon completion, SageMaker Processing will upload files in this directory to S3. The main script looks like below:\n",
    "```python\n",
    "def call_one_exe(a):\n",
    "    p = subprocess.Popen([\"./a.out\", a],stdout=subprocess.PIPE)\n",
    "    p_out, err= p.communicate()\n",
    "    output = p_out.decode(\"utf-8\")\n",
    "    return output.split(',')\n",
    "\n",
    "if __name__=='__main__':\n",
    "    #parse is only needed if we want to pass arg\n",
    "    parser = argparse.ArgumentParser()\n",
    "    #parser.add_argument('--num_thread', type=int, default=1000)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    print('Received arguments {}'.format(args))\n",
    "    \n",
    "    files = glob('/opt/ml/processing/input/*')\n",
    "    \n",
    "    for i, f in enumerate(files):\n",
    "        try:\n",
    "            data = pd.read_csv(f, header=None)\n",
    "            string = str(list(data.values.flat)).replace(' ','')[1:-1]\n",
    "            #string looks like 2,3,5,6,7,8. Space is removed. '[' and ']' are also removed.\n",
    "            predictions = call_one_exe(string)\n",
    "            \n",
    "            output_path = os.path.join('/opt/ml/processing/output', str(i)+'_out.csv')\n",
    "            print('Saving training features to {}'.format(output_path))\n",
    "            pd.DataFrame({'results':predictions}).to_csv(output_path, header=False, index=False)\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a processing job "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step would be to configure a processing job using the ScriptProcessor object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, os\n",
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "default_s3_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "\n",
    "client = boto3.client('sts')\n",
    "Account_number = client.get_caller_identity()['Account']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 sample data files are included in this demo. Each file contains 5000 raws of arbitrarily generated data. We first upload these 200 files to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = sagemaker_session.upload_data(path='./data_files', \n",
    "                                           bucket=default_s3_bucket, \n",
    "                                           key_prefix='data_for_inference_with_cpp_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a processing job using the Docker image and preprocessing script you just created. We pass the Amazon S3 input and output paths as arguments that are required by our preprocessing script to determine input and output location in Amazon S3. Here, we also specify the number of instances and instance type that will be used for the processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "script_processor = ScriptProcessor(command=['python3'],\n",
    "                image_uri=Account_number + '.dkr.ecr.us-east-1.amazonaws.com/cpp_processing:latest',\n",
    "                role=role,\n",
    "                instance_count=1,\n",
    "                base_job_name = 'run-exe-processing',\n",
    "                instance_type='ml.c5.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_location = os.path.join('s3://',default_s3_bucket, 'processing_output')\n",
    "\n",
    "script_processor.run(code='process_script.py',\n",
    "                     inputs=[ProcessingInput(\n",
    "                        source=input_data,\n",
    "                        destination='/opt/ml/processing/input')],\n",
    "                      outputs=[ProcessingOutput(source='/opt/ml/processing/output',\n",
    "                                               destination=output_bucket)]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the preprocessed dataset\n",
    "Take a look at a few rows of one dataset to make sure the preprocessing was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Top 5 rows from 1_out.csv')\n",
    "!aws s3 cp $output_location/0_out.csv - | head -n5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
